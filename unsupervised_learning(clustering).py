# -*- coding: utf-8 -*-
"""Unsupervised Learning(Clustering).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ObwTIhsOaID_VwE-g1f5qD1nfyKZuauY
"""

!pip install scikit-learn matplotlib seaborn numpy scipy pycaret

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE
from pycaret.clustering import *
import hdbscan
from sklearn.cluster import Birch
from sklearn.cluster import AffinityPropagation

iris = load_iris()
X = iris.data
y = iris.target

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X_scaled)

kmeans = KMeans(n_clusters=3, random_state=42)
kmeans_labels = kmeans.fit_predict(X_scaled)

agglo = AgglomerativeClustering(n_clusters=3)
agglo_labels = agglo.fit_predict(X_scaled)

dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_scaled)

from sklearn.cluster import MeanShift
mean_shift = MeanShift()
mean_shift_labels = mean_shift.fit_predict(X_scaled)

gmm = GaussianMixture(n_components=3)
gmm_labels = gmm.fit_predict(X_scaled)

from sklearn.cluster import SpectralClustering
spectral = SpectralClustering(n_clusters=3, affinity='nearest_neighbors')
spectral_labels = spectral.fit_predict(X_scaled)

birch = Birch(n_clusters=3)
birch_labels = birch.fit_predict(X_scaled)

!pip install scikit-learn-extra
!pip install scikit-learn

from sklearn_extra.cluster import KMedoids
kmedoids = KMedoids(n_clusters=3, random_state=42)
kmedoids_labels = kmedoids.fit_predict(X_scaled)

affinity = AffinityPropagation(random_state=42)
affinity_labels = affinity.fit_predict(X_scaled)

!pip install minisom
from minisom import MiniSom
import numpy as np

som = MiniSom(3, 3, 4, sigma=0.5, learning_rate=0.5)

som.train(X_scaled, 500)

win_map = som.win_map(X_scaled)

som_labels = np.array([som.winner(x) for x in X_scaled])

print(som_labels)

from sklearn.cluster import OPTICS
optics = OPTICS(min_samples=5)
optics_labels = optics.fit_predict(X_scaled)

from sklearn.cluster import AgglomerativeClustering
hierarchical = AgglomerativeClustering(n_clusters=3)
hierarchical_labels = hierarchical.fit_predict(X_scaled)

!pip install fuzzy-c-means

from fcmeans import FCM
fcm = FCM(n_clusters=3)
fcm.fit(X_scaled)
fcm_labels = fcm.predict(X_scaled)

!pip install hdbscan
import hdbscan
hdbscan_model = hdbscan.HDBSCAN(min_samples=5)
hdbscan_labels = hdbscan_model.fit_predict(X_scaled)

from minisom import MiniSom
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Train the SOM with PCA-reduced data (X_pca, which has 2 features)
som = MiniSom(3, 3, 2, sigma=0.5, learning_rate=0.5)  # Change '4' to '2' here
som.train(X_pca, 500)

# Get the labels by finding the winning node for each input
som_labels = np.array([som.winner(x)[0] for x in X_pca])

# Visualize the SOM clustering result
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=som_labels, palette="Set1", legend="full")
plt.title("Self-Organizing Maps (SOM) Clustering")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.show()

def plot_clusters(X, labels, title):
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels, palette="Set1", legend="full")
    plt.title(title)
    plt.show()

plot_clusters(X_pca, kmeans_labels, "K-Means Clustering")
plot_clusters(X_pca, agglo_labels, "Agglomerative Clustering")
plot_clusters(X_pca, dbscan_labels, "DBSCAN Clustering")
plot_clusters(X_pca, mean_shift_labels, "Mean-Shift Clustering")
plot_clusters(X_pca, gmm_labels, "Gaussian Mixture Model")
plot_clusters(X_pca, spectral_labels, "Spectral Clustering")
plot_clusters(X_pca, birch_labels, "Birch Clustering")
plot_clusters(X_pca, kmedoids_labels, "K-Medoids Clustering")
plot_clusters(X_pca, affinity_labels, "Affinity Propagation")
plot_clusters(X_pca, optics_labels, "OPTICS Clustering")
plot_clusters(X_pca, hierarchical_labels, "Hierarchical Clustering")
plot_clusters(X_pca, fcm_labels, "Fuzzy C-Means Clustering")

def silhouette_evaluation(labels):
    score = silhouette_score(X_scaled, labels)
    print(f"Silhouette Score: {score}")

silhouette_evaluation(kmeans_labels)
silhouette_evaluation(agglo_labels)
silhouette_evaluation(dbscan_labels)
silhouette_evaluation(mean_shift_labels)
silhouette_evaluation(gmm_labels)
silhouette_evaluation(spectral_labels)
silhouette_evaluation(birch_labels)
silhouette_evaluation(kmedoids_labels)
silhouette_evaluation(affinity_labels)
silhouette_evaluation(som_labels)
silhouette_evaluation(optics_labels)
silhouette_evaluation(hierarchical_labels)
silhouette_evaluation(fcm_labels)
silhouette_evaluation(dp_labels)
silhouette_evaluation(cw_labels)
silhouette_evaluation(lda_labels)
silhouette_evaluation(hdbscan_labels)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris
import hdbscan
from sklearn.decomposition import PCA

iris = load_iris()
X = iris.data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

lda = LatentDirichletAllocation(n_components=3, random_state=42)
lda_labels = lda.fit_transform(X)

lda_labels = np.argmax(lda_labels, axis=1)

hdbscan_model = hdbscan.HDBSCAN(min_samples=5)
hdbscan_labels = hdbscan_model.fit_predict(X_pca)

def plot_clusters(X, labels, title):
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels, palette="Set1", legend="full", marker='o')
    plt.title(title)
    plt.xlabel("PCA Component 1")
    plt.ylabel("PCA Component 2")
    plt.show()

plot_clusters(X_pca, lda_labels, "Latent Dirichlet Allocation (LDA)")

hdbscan_labels_filtered = np.where(hdbscan_labels == -1, np.nan, hdbscan_labels)
plot_clusters(X_pca, hdbscan_labels_filtered, "HDBSCAN Clustering")

from sklearn.decomposition import LatentDirichletAllocation
from sklearn.datasets import load_iris
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

iris = load_iris()
X = iris.data

lda = LatentDirichletAllocation(n_components=3, random_state=42)
lda_labels = lda.fit_transform(X)

print("Topic Distributions (samples x topics):")
print(lda_labels)

plt.figure(figsize=(8, 6))

scatter = sns.scatterplot(x=lda_labels[:, 0], y=lda_labels[:, 1], hue=lda_labels.argmax(axis=1), palette="Set1", legend="full")

plt.title("LDA Topic Distributions (First 2 Components)")
plt.xlabel("Topic 1")
plt.ylabel("Topic 2")

norm = plt.Normalize(vmin=lda_labels.min(), vmax=lda_labels.max())
sm = plt.cm.ScalarMappable(cmap="Set1", norm=norm)
sm.set_array([])
plt.colorbar(sm, label="Cluster ID")

plt.show()

!pip install chinese-whispers

import numpy as np
import random
from sklearn.metrics import pairwise_distances
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

def chinese_whispers(X, n_iter=100):

    distance_matrix = pairwise_distances(X)

    n_samples = X.shape[0]
    labels = np.arange(n_samples)

    for _ in range(n_iter):

        for i in range(n_samples):
            neighbors = np.argsort(distance_matrix[i])[:5]
            neighbor_labels = labels[neighbors]

            labels[i] = random.choice(neighbor_labels)

    return labels

from sklearn.datasets import load_iris

iris = load_iris()
X = iris.data

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

cw_labels = chinese_whispers(X_scaled)

plt.figure(figsize=(8, 6))
scatter = sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=cw_labels, palette="Set1", legend="full")
plt.title("Chinese Whispers Clustering (Manual Implementation)")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")

plt.colorbar(scatter.collections[0], label="Cluster ID")

plt.show()

print(cw_labels)

!pip install scikit-learn matplotlib seaborn

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import pairwise_distances
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris

def calculate_density(distance_matrix, cutoff=1.0):
    density = np.zeros(len(distance_matrix))
    for i in range(len(distance_matrix)):
        density[i] = np.sum(np.exp(-distance_matrix[i] ** 2 / (2 * cutoff ** 2)))
    return density

def calculate_min_distance(distance_matrix, density):
    min_distance = np.zeros(len(distance_matrix))
    for i in range(len(distance_matrix)):
        higher_density_points = density > density[i]
        if np.any(higher_density_points):
            min_distance[i] = np.min(distance_matrix[i, higher_density_points])
        else:
            min_distance[i] = np.inf
    return min_distance

def density_peaks_clustering(X, cutoff=1.0):

    distance_matrix = pairwise_distances(X)

    density = calculate_density(distance_matrix, cutoff)

    min_distance = calculate_min_distance(distance_matrix, density)

    sorted_indices = np.argsort(density)[::-1]
    labels = -np.ones(len(X))

    cluster_label = 0
    for i in sorted_indices:
        if density[i] > 0 and min_distance[i] > cutoff:
            labels[i] = cluster_label
            cluster_label += 1
        else:
            labels[i] = cluster_label - 1

    return labels, density, min_distance

iris = load_iris()
X = iris.data

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

dp_labels, density, min_distance = density_peaks_clustering(X_scaled)

plt.figure(figsize=(8, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=dp_labels, cmap="Set1")
plt.title("Density Peaks Clustering (Manual Implementation)")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.colorbar(label="Cluster ID")
plt.show()